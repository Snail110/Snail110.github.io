<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>NFM模型理论和实践 | 一碗竹叶青</title><meta name="description" content="NFM模型理论和实践"><meta name="keywords" content="DNN,推荐系统"><meta name="author" content="julianxu"><meta name="copyright" content="julianxu"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="http://ta.qq.com"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="NFM模型理论和实践"><meta name="twitter:description" content="NFM模型理论和实践"><meta name="twitter:image" content="https://img.phb123.com/uploads/allimg/180122/24-1P122155022A0.jpg"><meta property="og:type" content="article"><meta property="og:title" content="NFM模型理论和实践"><meta property="og:url" content="http://snail110.github.io/2019/12/22/NFM/"><meta property="og:site_name" content="一碗竹叶青"><meta property="og:description" content="NFM模型理论和实践"><meta property="og:image" content="https://img.phb123.com/uploads/allimg/180122/24-1P122155022A0.jpg"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://snail110.github.io/2019/12/22/NFM/"><link rel="prev" title="GBDT+LR模型理论和实践" href="http://snail110.github.io/2019/12/22/GBDT+LR%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90/"><link rel="next" title="PNN模型理论和实践" href="http://snail110.github.io/2019/12/22/PNN%E6%A8%A1%E5%9E%8B%E7%90%86%E8%AE%BA%E5%92%8C%E5%AE%9E%E8%B7%B5/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5/js/md5.min.js"></script><script src="https://tajs.qq.com/stats?sId=66547258" charset="UTF-8"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"></head><body><canvas class="fireworks"></canvas><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/user.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">12</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">4</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 档案室</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/playlist/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#概述"><span class="toc-number">1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0-介绍"><span class="toc-number">2.</span> <span class="toc-text">0.介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#公式（2）"><span class="toc-number">2.1.</span> <span class="toc-text">公式（2）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#公式（3）"><span class="toc-number">2.2.</span> <span class="toc-text">公式（3）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-代码复现"><span class="toc-number">3.</span> <span class="toc-text">2.代码复现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-总结"><span class="toc-number">4.</span> <span class="toc-text">3.总结</span></a></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(https://img.phb123.com/uploads/allimg/180122/24-1P122155022A0.jpg)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">一碗竹叶青</a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 档案室</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/playlist/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">NFM模型理论和实践</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2019-12-22 12:39:04"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2019-12-22</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-04-23 08:46:45"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-04-23</span></time></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>评论数:</span><a href="/2019/12/22/NFM/#post-comment"><span class="gitalk-comment-count comment-count"></span></a></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><meta name="referrer" content="no-referrer">
TensorFlow 2.0 implementation of NFM
<a id="more"></a>

<p>参考文章：<br>TensorFlow 2.0 implementation of NFM<br>Reference:<br>Neural Factorization Machines for Sparse Predictive Analytics<br><a href="https://www.jianshu.com/p/4e65723ee632" target="_blank" rel="noopener">https://www.jianshu.com/p/4e65723ee632</a></p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>该系列主要是复现一些经典的网络结构与顶会论文的网络结构，我一开始看论文，以为看到网络结构和了解结构原理后，就完全学到了这篇论文的精髓，谁知，等到自己想要用这个网络结构时，无法打通理解与代码复现的这一步，这就导致我在科研或者工作时的束手无措，因此，我就决定探究如何将一篇论文中的结构和论文中的公式用代码的形式复现出来。<br>深度学习框架：tensorflow2.0 ，numpy。<br>语言：python。<br>复现的代码全部在：<a href="https://github.com/Snail110/recsys。" target="_blank" rel="noopener">https://github.com/Snail110/recsys。</a></p>
<h2 id="0-介绍"><a href="#0-介绍" class="headerlink" title="0.介绍"></a>0.介绍</h2><p>FM和深度网络DNN的结合也就成为了CTR预估问题中主流的方法。有关FM和DNN的结合有两种主流的方法，并行结构和串行结构。两种结构的理解以及实现如下表所示：</p>
<table>
<thead>
<tr>
<th>结构</th>
<th>描述</th>
<th>常见模型</th>
</tr>
</thead>
<tbody><tr>
<td>并行结构</td>
<td>FM部分和DNN部分分开计算，只在输出层进行一次融合得到结果</td>
<td>DeepFM，DCN，Wide&amp;Deep</td>
</tr>
<tr>
<td>串行结构</td>
<td>将FM的一次项和二次项结果(或其中之一)作为DNN部分的输入，经DNN得到最终结果</td>
<td>PNN,NFM,AFM</td>
</tr>
<tr>
<td>## 1.网络结构</td>
<td></td>
<td></td>
</tr>
<tr>
<td>该部分主要是将论文中公式与结构图对应起来，理解每一个公式的含义以及网络结构图中每一部分的输入输出。</td>
<td></td>
<td></td>
</tr>
<tr>
<td><img src="/" class="lazyload" data-src="https://img-blog.csdnimg.cn/20200208193531463.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzM4MTI3MTYy,size_16,color_FFFFFF,t_70"  alt="模型图"></td>
<td></td>
<td></td>
</tr>
<tr>
<td>首先，当你看完一篇论文并理解了论文的主要思想后，需要尝试着将网络结构与论文中的每一步的数学公式一一对应上，在心中或者图片上协商每一个环节的数学公式，然后考虑用深度学习框架来实现。</td>
<td></td>
<td></td>
</tr>
<tr>
<td>首先这篇论文中有数学公式(1)，（2）,（3）对应着网络模型。</td>
<td></td>
<td></td>
</tr>
<tr>
<td>然后需要一步一步的将公式对应到网络模型中，</td>
<td></td>
<td></td>
</tr>
<tr>
<td>### 公式（1）</td>
<td></td>
<td></td>
</tr>
<tr>
<td><img src="/" class="lazyload" data-src="https://img-blog.csdnimg.cn/20200208193517689.PNG"  alt="在这里插入图片描述"></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>输出层是若干层的全连接层，经过多次的deep layers 获取深度特征。</p>
<h3 id="公式（2）"><a href="#公式（2）" class="headerlink" title="公式（2）"></a>公式（2）</h3><p>Bi-Interaction Layer名字挺高大上的，其实它就是计算FM中的二次项的过程，因此得到的向量维度就是我们的Embedding的维度。最终的结果是：<br><img src="/" class="lazyload" data-src="https://img-blog.csdnimg.cn/20200208193838714.PNG"  alt></p>
<h3 id="公式（3）"><a href="#公式（3）" class="headerlink" title="公式（3）"></a>公式（3）</h3><p>Hidden Layers就是我们的DNN部分，将Bi-Interaction Layer得到的结果接入多层的神经网络进行训练，从而捕捉到特征之间复杂的非线性关系。</p>
<p>在进行多层训练之后，将最后一层的输出求和同时加上一次项和偏置项，就得到了我们的预测输出：<br><img src="/" class="lazyload" data-src="https://img-blog.csdnimg.cn/20200208194014810.PNG"  alt="在这里插入图片描述"></p>
<h2 id="2-代码复现"><a href="#2-代码复现" class="headerlink" title="2.代码复现"></a>2.代码复现</h2><p>该部分主要是按照网络结构图，用代码的方式实现。在代码实现的过程中，我们需要紧紧结合数学公式体会其中的含义以及如何用代码来实现这些数学公式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiInteraction</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, Units=<span class="number">1</span>, **kwargs)</span>:</span></span><br><span class="line">        self.units = Units</span><br><span class="line">        super(BiInteraction, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        input_dim = input_shape[<span class="number">2</span>]</span><br><span class="line">        <span class="comment"># self.W = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', trainable=True)</span></span><br><span class="line">        <span class="comment"># self.b = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', trainable=True)</span></span><br><span class="line">        self.linearlayer = tf.keras.layers.Dense(input_dim, activation=<span class="string">'relu'</span>, use_bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="comment"># sum-square-part</span></span><br><span class="line">        self.summed_features_emb = tf.reduce_sum(input,<span class="number">1</span>) <span class="comment"># None * K</span></span><br><span class="line">        <span class="comment"># print("self.summed_features_emb:",self.summed_features_emb.get_shape())</span></span><br><span class="line">        self.summed_features_emb_square = tf.square(self.summed_features_emb) <span class="comment"># None * K</span></span><br><span class="line">        <span class="comment"># square-sum-part</span></span><br><span class="line">        self.squared_features_emb = tf.square(input)</span><br><span class="line">        self.squared_sum_features_emb = tf.reduce_sum(self.squared_features_emb,<span class="number">1</span>) <span class="comment"># None * K</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># second order</span></span><br><span class="line">        self.y_second_order = <span class="number">0.5</span> * tf.subtract(self.summed_features_emb_square,self.squared_sum_features_emb) <span class="comment"># None * K</span></span><br><span class="line">        <span class="comment"># print("y_second_order:",self.y_second_order.get_shape()) # 128 * 10</span></span><br><span class="line">        output = self.linearlayer(self.y_second_order)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NFM</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_feat, num_field, dropout_deep, deep_layer_sizes, embedding_size=<span class="number">10</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.num_feat = num_feat  <span class="comment"># F =features nums</span></span><br><span class="line">        self.num_field = num_field  <span class="comment"># N =fields of a feature</span></span><br><span class="line">        self.dropout_deep = dropout_deep</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Embedding 这里采用embeddings层因此大小为F* M F为特征数量，M为embedding的维度</span></span><br><span class="line">        feat_embeddings = tf.keras.layers.Embedding(num_feat, embedding_size,</span><br><span class="line">                                                    embeddings_initializer=<span class="string">'uniform'</span>)  <span class="comment"># F * M</span></span><br><span class="line">        self.feat_embeddings = feat_embeddings</span><br><span class="line"></span><br><span class="line">        <span class="comment"># fc layer</span></span><br><span class="line">        self.deep_layer_sizes = deep_layer_sizes</span><br><span class="line">        <span class="comment"># 神经网络方面的参数</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(deep_layer_sizes)):</span><br><span class="line">            setattr(self, <span class="string">'dense_'</span> + str(i), tf.keras.layers.Dense(deep_layer_sizes[i]))</span><br><span class="line">            setattr(self, <span class="string">'batchNorm_'</span> + str(i), tf.keras.layers.BatchNormalization())</span><br><span class="line">            setattr(self, <span class="string">'activation_'</span> + str(i), tf.keras.layers.Activation(<span class="string">'relu'</span>))</span><br><span class="line">            setattr(self, <span class="string">'dropout_'</span> + str(i), tf.keras.layers.Dropout(dropout_deep[i]))</span><br><span class="line">        self.bilayer = BiInteraction(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># last layer</span></span><br><span class="line">        self.fc = tf.keras.layers.Dense(<span class="number">1</span>, activation=<span class="literal">None</span>, use_bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.linearlayer = tf.keras.layers.Dense(deep_layer_sizes[<span class="number">-1</span>], activation=<span class="string">'relu'</span>, use_bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, feat_index, feat_value)</span>:</span></span><br><span class="line">        <span class="comment"># call函数接收输入变量</span></span><br><span class="line">        <span class="comment"># embedding part  feat_index = inputs为输入 feat_embeddings为一个layer。</span></span><br><span class="line">        feat_embedding_0 = self.feat_embeddings(feat_index)  <span class="comment"># Batch * N * M</span></span><br><span class="line">        <span class="comment">#         print(feat_value.get_shape())</span></span><br><span class="line">        feat_embedding = tf.einsum(<span class="string">'bnm,bn-&gt;bnm'</span>, feat_embedding_0, feat_value)</span><br><span class="line"></span><br><span class="line">        y_deep = self.attentionlayer(feat_embedding)</span><br><span class="line">        y_linear = self.linearlayer(tf.reduce_sum(feat_embedding,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.deep_layer_sizes)):</span><br><span class="line">            y_deep = getattr(self, <span class="string">'dense_'</span> + str(i))(y_deep)</span><br><span class="line">            y_deep = getattr(self, <span class="string">'batchNorm_'</span> + str(i))(y_deep)</span><br><span class="line">            y_deep = getattr(self, <span class="string">'activation_'</span> + str(i))(y_deep)</span><br><span class="line">            y_deep = getattr(self, <span class="string">'dropout_'</span> + str(i))(y_deep)</span><br><span class="line">        y = y_deep + y_linear</span><br><span class="line">        output = self.fc(y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>


<h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3.总结"></a>3.总结</h2><p>你看，通过这样一步步将公式与代码对应起来，就好实现多了，对于不同的计算公式采用不同的函数需要多看文档，这样才可以选用正确的api。<br>最后，如果需要获取全部代码，请看下我的github上仓库：<a href="https://github.com/Snail110/recsys" target="_blank" rel="noopener">https://github.com/Snail110/recsys</a><br>这里面是用tensorflow2.0框架来写。如果觉得我实现的还不错，记得给我一个星星哦。</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">julianxu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://snail110.github.io/2019/12/22/NFM/">http://snail110.github.io/2019/12/22/NFM/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://Snail110.github.io" target="_blank">一碗竹叶青</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/DNN/">DNN</a><a class="post-meta__tags" href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a></div><div class="post_share"><div class="social-share" data-image="http://img.ylq.com/2017/0316/20170316035833715.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2019/12/22/GBDT+LR%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90/"><img class="prev_cover lazyload" data-src="https://img.phb123.com/uploads/allimg/180122/24-1P122154Z2Z9.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">GBDT+LR模型理论和实践</div></div></a></div><div class="next-post pull_right"><a href="/2019/12/22/PNN%E6%A8%A1%E5%9E%8B%E7%90%86%E8%AE%BA%E5%92%8C%E5%AE%9E%E8%B7%B5/"><img class="next_cover lazyload" data-src="https://img.phb123.com/uploads/allimg/180122/24-1P122154525A5.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">PNN模型理论和实践</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/12/22/DCN模型理论和实践/" title="DCN模型理论和实践"><img class="relatedPosts_cover lazyload"data-src="https://img.phb123.com/uploads/allimg/180122/24-1P122154S0230.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2019-12-22</div><div class="relatedPosts_title">DCN模型理论和实践</div></div></a></div><div class="relatedPosts_item"><a href="/2019/12/22/GBDT+LR算法解析/" title="GBDT+LR模型理论和实践"><img class="relatedPosts_cover lazyload"data-src="https://img.phb123.com/uploads/allimg/180122/24-1P122154Z2Z9.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2019-12-22</div><div class="relatedPosts_title">GBDT+LR模型理论和实践</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/23/Deep Neural Network for YouTube Recommendation/" title="YouTube视频推荐的DNN算法"><img class="relatedPosts_cover lazyload"data-src="http://pic1.win4000.com/pic/3/3d/e0aa891379.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-23</div><div class="relatedPosts_title">YouTube视频推荐的DNN算法</div></div></a></div><div class="relatedPosts_item"><a href="/2019/12/22/PNN模型理论和实践/" title="PNN模型理论和实践"><img class="relatedPosts_cover lazyload"data-src="https://img.phb123.com/uploads/allimg/180122/24-1P122154525A5.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2019-12-22</div><div class="relatedPosts_title">PNN模型理论和实践</div></div></a></div><div class="relatedPosts_item"><a href="/2019/12/22/MLR/" title="MLR模型理论和实践"><img class="relatedPosts_cover lazyload"data-src="https://img.phb123.com/uploads/allimg/180122/24-1P122154305S5.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2019-12-22</div><div class="relatedPosts_title">MLR模型理论和实践</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/23/Wide&Deep模型网络结构/" title="Wide&Deep"><img class="relatedPosts_cover lazyload"data-src="http://dingyue.ws.126.net/lyFlfhBGdx8qHeO6nyMzTbcAXTuKFhbUwrsoh8WgxCWQX1579239824693compressflag.jpeg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-23</div><div class="relatedPosts_title">Wide&Deep</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: '5f0d5e0bdf7f8aeaaf56',
  clientSecret: 'dcdb00026cec17a372afde717c5b943294207dbf',
  repo: '',
  owner: '',
  admin: [''],
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN',
  perPage: 10,
  distractionFreeMode: false,
  pagerDirection: 'last',
  createIssueManually: false,
  updateCountCallback: commentCount
})
gitalk.render('gitalk-container')

function commentCount(n){
  try {
    document.getElementsByClassName('gitalk-comment-count')[0].innerHTML= n
  } catch (e) {
    return false
  }
}</script></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By julianxu</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">2</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script id="ribbon_piao" mobile="false" src="/js/third-party/piao.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>